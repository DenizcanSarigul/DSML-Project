{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"gsfFcpZffr8i"},"outputs":[],"source":["#%%capture\n","#!pip install transformers[torch]\n","#!pip install datasets\n","#!pip install pyarrow\n","#!pip install evaluate"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"-wFF1JMWgOjb"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"TvdQvt_WgTp7"},"outputs":[],"source":["my_data = pd.read_csv(\"training_data.csv\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1701098812719,"user":{"displayName":"philip hubscher","userId":"13683277909824048792"},"user_tz":-60},"id":"zdm1DupDglK7","outputId":"d07aa179-df5e-4a26-8b8e-c6da34d3b4f8"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentence</th>\n","      <th>difficulty</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Les coûts kilométriques réels peuvent diverger...</td>\n","      <td>C1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Le bleu, c'est ma couleur préférée mais je n'a...</td>\n","      <td>A1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Le test de niveau en français est sur le site ...</td>\n","      <td>A1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Est-ce que ton mari est aussi de Boston?</td>\n","      <td>A1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Dans les écoles de commerce, dans les couloirs...</td>\n","      <td>B1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                           sentence difficulty\n","0   0  Les coûts kilométriques réels peuvent diverger...         C1\n","1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n","2   2  Le test de niveau en français est sur le site ...         A1\n","3   3           Est-ce que ton mari est aussi de Boston?         A1\n","4   4  Dans les écoles de commerce, dans les couloirs...         B1"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["my_data.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1701098812719,"user":{"displayName":"philip hubscher","userId":"13683277909824048792"},"user_tz":-60},"id":"oq6aYsRetcXw","outputId":"9a345431-6fc9-45c0-f11f-22ad144af378"},"outputs":[{"data":{"text/plain":["(4800, 3)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["my_data.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"LgYvkW5EhMQA"},"outputs":[],"source":["from transformers import AutoModel, AutoTokenizer\n","from transformers import CamembertTokenizer\n","from typing import Dict"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3PD2Sv22iFSB"},"outputs":[],"source":["%%capture\n","tokenizer = AutoTokenizer.from_pretrained('camembert-base')"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"iGuzAWflsFt3"},"outputs":[],"source":["def process_data(row) -> Dict:\n","    # Clean the text\n","    text = row['sentence']\n","    text = str(text)\n","    text = ' '.join(text.split())\n","\n","    # Get tokens\n","    encodings = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n","\n","    # Convert difficulty labels to integers\n","    difficulty_mapping = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n","    label = difficulty_mapping.get(row['difficulty'], 0)\n","\n","    encodings['label'] = label\n","    encodings['text'] = text\n","\n","    return encodings"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1701098821643,"user":{"displayName":"philip hubscher","userId":"13683277909824048792"},"user_tz":-60},"id":"JdosNrkHsiO5","outputId":"9c5f3978-2dd8-4539-f5f2-62fbe0dc8d2c"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [5, 1196, 26, 291, 27, 415, 1946, 30, 99, 8, 19171, 197, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'text': 'Est-ce que ton mari est aussi de Boston?'}\n"]}],"source":["#test\n","print(process_data({'sentence': \"Est-ce que ton mari est aussi de Boston?\", 'difficulty': 'A1'}))"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"vWTE0yVYtIFX"},"outputs":[],"source":["# Store the encodings into an array to generate dataset\n","processed_data = []\n","\n","for i in range(len(my_data[:4800])):\n","        processed_data.append(process_data(my_data.iloc[i]))"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"rSKfbK2vt0RW"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"xEYHvQaktxbQ"},"outputs":[],"source":["new_df = pd.DataFrame(processed_data)\n","\n","train_df, valid_df = train_test_split(new_df,test_size=0.2,random_state=2022)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"-XcrqSS7w73h"},"outputs":[],"source":["import pyarrow as pa\n","from datasets import Dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"wL8MkeElxp6S"},"outputs":[],"source":["train_hg = Dataset(pa.Table.from_pandas(train_df))\n","valid_hg = Dataset(pa.Table.from_pandas(valid_df))"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1701098829411,"user":{"displayName":"philip hubscher","userId":"13683277909824048792"},"user_tz":-60},"id":"SNXVBs-2OH0e","outputId":"2daf9be7-cb76-4d10-b320-c74d3b5fc5f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'datasets.arrow_dataset.Dataset'>\n"]}],"source":["print(type(train_hg))"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"TaSNQgqlxrm_"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":25889,"status":"ok","timestamp":1701098855295,"user":{"displayName":"philip hubscher","userId":"13683277909824048792"},"user_tz":-60},"id":"Crv7YYWOxwNe","outputId":"7c9800b2-aab7-4ed5-cc16-d6ac844f949c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["%%capture\n","model = AutoModelForSequenceClassification.from_pretrained('camembert-base',num_labels=6)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"PQH_jPC1FWmo"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"qD2NUwFfFZnX"},"outputs":[],"source":["\n","def compute_metrics(p):\n","    predictions, labels = p.predictions, p.label_ids\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    accuracy = accuracy_score(labels, predictions)\n","    precision = precision_score(labels, predictions, average='weighted')\n","    recall = recall_score(labels, predictions, average='weighted')\n","    f1 = f1_score(labels, predictions, average='weighted')\n","\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1,\n","    }"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"1W0HRMU2y9OG"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","import accelerate"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"jLtj20R9yK19"},"outputs":[],"source":["training_args = TrainingArguments(output_dir=\"./result\", evaluation_strategy=\"epoch\")\n","\n","trainer = Trainer(model=model,args=training_args,train_dataset=train_hg,eval_dataset=valid_hg,tokenizer=tokenizer,compute_metrics=compute_metrics)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205},"executionInfo":{"elapsed":1156551,"status":"ok","timestamp":1701102422444,"user":{"displayName":"philip hubscher","userId":"13683277909824048792"},"user_tz":-60},"id":"wF_EGWQoyxuG","outputId":"b27fecd0-124f-4735-b77f-8e14e616b97f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd0f66f906294f639899c51d93b603f6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\DENIZ\\Documents\\GitHub\\DSML-Project\\BertModels\\Local\\Bert_train_model_V2.ipynb Cellule 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DENIZ/Documents/GitHub/DSML-Project/BertModels/Local/Bert_train_model_V2.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n","File \u001b[1;32mc:\\Users\\DENIZ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\DENIZ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1866\u001b[0m ):\n\u001b[0;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n","File \u001b[1;32mc:\\Users\\DENIZ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2734\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2732\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m   2733\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2734\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[0;32m   2736\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n","File \u001b[1;32mc:\\Users\\DENIZ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:1989\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1987\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1988\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1989\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32mc:\\Users\\DENIZ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n","File \u001b[1;32mc:\\Users\\DENIZ\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":199},"executionInfo":{"elapsed":27957,"status":"ok","timestamp":1701102450389,"user":{"displayName":"philip hubscher","userId":"13683277909824048792"},"user_tz":-60},"id":"DX0JjFUJMjLG","outputId":"77fbd734-5c6c-4938-989f-bd9baa7e2ed4"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [120/120 00:28]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 1.926348328590393,\n"," 'eval_accuracy': 0.55625,\n"," 'eval_precision': 0.5885482243240117,\n"," 'eval_recall': 0.55625,\n"," 'eval_f1': 0.5548746000654246,\n"," 'eval_runtime': 28.6108,\n"," 'eval_samples_per_second': 33.554,\n"," 'eval_steps_per_second': 4.194,\n"," 'epoch': 3.0}"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FxF-AcGMzct"},"outputs":[],"source":["model1.save_pretrained('/content/drive/MyDrive/Data Science and Machine Learning/Project_Data_Science/train_model_bert_V2/')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO7bZaII2S/LO02+t2SIqPg","gpuType":"T4","provenance":[{"file_id":"1NKqDDrZ26WaXY2w9XVuEi2PlyhwzKMmW","timestamp":1700777988142}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":0}
