{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gsfFcpZffr8i","trusted":true},"outputs":[],"source":["%%capture\n","!pip install transformers[torch]\n","!pip install datasets\n","!pip install pyarrow\n","!pip install evaluate\n","!pip install --upgrade -q wandb\n","\n","#!python -m spacy download fr_core_news_sm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wFF1JMWgOjb","trusted":true},"outputs":[],"source":["%%capture\n","import pandas as pd\n","from transformers import AutoModel, AutoTokenizer\n","from transformers import FlaubertModel, FlaubertTokenizer#CamembertTokenizer\n","from typing import Dict\n","import pyarrow as pa\n","from datasets import Dataset\n","from transformers import AutoModelForSequenceClassification\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from transformers import TrainingArguments, Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvdQvt_WgTp7","trusted":true},"outputs":[],"source":["my_data = pd.read_csv(\"/kaggle/input/textes/training_data.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdm1DupDglK7","outputId":"d5cbc5cc-3645-41ff-a523-4edbc9f8b276","trusted":true},"outputs":[],"source":["my_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oq6aYsRetcXw","outputId":"89b6c976-228b-4c89-b050-07a94c70f62c","trusted":true},"outputs":[],"source":["my_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWptDYu3wlNT","trusted":true},"outputs":[],"source":["\n","# Number of words in each sentence\n","number_data = []\n","\n","# Iterate through your DataFrame rows\n","for index, row in my_data.iterrows():\n","   \n","    num_words = len(row['sentence'].split())\n","\n","    # Append the result to the processed_data list\n","    number_data.append({'sentence': row['sentence'], 'difficulty': row['difficulty'], 'num_words': num_words})\n","\n","# Create a new DataFrame from the processed_data list\n","new_df = pd.DataFrame(number_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTIa3JJ9w4lu","outputId":"27d69fca-ee8d-4444-fd0d-6149b6bdc52f","trusted":true},"outputs":[],"source":["# Group by 'difficulty' and calculate the mean number of words\n","avg_words_by_difficulty = new_df.groupby('difficulty')['num_words'].mean()\n","\n","# number of words by difficulty\n","print(avg_words_by_difficulty)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PD2Sv22iFSB","trusted":true},"outputs":[],"source":["%%capture\n","tokenizer = AutoTokenizer.from_pretrained('camembert-base',do_lower_case=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_L8YqjgceLw","trusted":true},"outputs":[],"source":["## The code to create the sentence without the numbers. We didn't use it because it didn't improve the results. \n","\n","\n","# import re\n","#from typing import Dict\n","#\n","#def remove_numbers(text):\n","#    # Use regular expression to remove numbers\n","#    text_without_numbers = re.sub(r'\\d+', '', text)\n","#    return text_without_numbers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkjrXX6pBqxi","trusted":true},"outputs":[],"source":["## The code to substracts the proper names. We didn't use it because it didn't improve the results.\n","\n","\n","#import spacy\n","#\n","## Load the spaCy French language model\n","#nlp = spacy.load('fr_core_news_sm')\n","#\n","#def remove_proper_names(text):\n","#    doc = nlp(text)\n","#    # Remove entities (proper names)\n","#    text_without_entities = ' '.join([token.text if not token.ent_type_ else '' for token in doc])\n","#    return text_without_entities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGuzAWflsFt3","trusted":true},"outputs":[],"source":["def process_data(row) -> Dict:\n","    # Clean the text\n","    text = row['sentence']\n","\n","    # Get tokens\n","    encodings = tokenizer(text, truncation=True, max_length=512)\n","\n","    # Convert difficulty labels to integers\n","    difficulty_mapping = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4, 'C2': 5}\n","    label = difficulty_mapping.get(row['difficulty'], 0)\n","\n","    encodings['label'] = label\n","    encodings['text'] = text\n","\n","    return encodings\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JdosNrkHsiO5","outputId":"a6a5349a-2c96-43a2-a53b-fa82e8d3f205","trusted":true},"outputs":[],"source":["#test\n","print(process_data({'sentence': \"En fait, je trouve que l'Ã©ducation est une bonne chose mais il ne faut pas .\",'difficulty': 'B1'}))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWTE0yVYtIFX","trusted":true},"outputs":[],"source":["# Store the encodings into an array to generate dataset\n","processed_data = []\n","\n","for i in range(len(my_data[:4800])):\n","        processed_data.append(process_data(my_data.iloc[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rSKfbK2vt0RW","trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","#from transformers import DataCollatorWithPadding\n","#import evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEYHvQaktxbQ","trusted":true},"outputs":[],"source":["new_df = pd.DataFrame(processed_data)\n","\n","train_df, valid_df = train_test_split(new_df,test_size=0.15,random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wL8MkeElxp6S","trusted":true},"outputs":[],"source":["train_hg = Dataset(pa.Table.from_pandas(train_df))\n","valid_hg = Dataset(pa.Table.from_pandas(valid_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Crv7YYWOxwNe","trusted":true},"outputs":[],"source":["%%capture\n","model1 = AutoModelForSequenceClassification.from_pretrained(\"camembert-base\",num_labels=6)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNXVBs-2OH0e","trusted":true},"outputs":[],"source":["## We tried the droopout technique to prevent overfitting but we didn't notice any visible improvement in the results so we didn't use it.\n","\n","#from torch.nn import Dropout\n","##we are adding a droupout layer to prevent over fitting \n","#model1.dropout = Dropout(0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qD2NUwFfFZnX","trusted":true},"outputs":[],"source":["\n","def compute_metrics(p):\n","\n","    predictions, labels = p.predictions, p.label_ids\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    accuracy = accuracy_score(labels, predictions)\n","    precision = precision_score(labels, predictions, average='weighted')\n","    recall = recall_score(labels, predictions, average='weighted')\n","    f1 = f1_score(labels, predictions, average='weighted')\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1,\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLtj20R9yK19","trusted":true},"outputs":[],"source":["# We let all the diffrent parameters as default except the number of epochs which we set to 10. \n","# We tried to change the learning rate and other parameters but we get our best results without changing them (with the base values). \n","# All the parameters are commented below are those we tried to change to get best results but we didn't get any improvement.\n","\n","training_args = TrainingArguments(\n","    output_dir=\"/kaggle/working/results\",\n","    num_train_epochs=10,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    \n","    #learning_rate=3e-5, #5e-5 by default\n","    #per_device_train_batch_size=16,\n","    #per_device_eval_batch_size=16,\n","    #fp16=True,\n","    #fp16_opt_level=\"O2\",\n","    #weight_decay=0.01,\n","    #metric_for_best_model=\"accuracy\",\n","    #warmup_steps=200,\n","    #gradient_accumulation_steps = 4 \n","    \n","    )\n","\n","trainer = Trainer(model=model1,\n","                  args=training_args,\n","                  train_dataset=train_hg,\n","                  eval_dataset=valid_hg,\n","                  tokenizer=tokenizer,\n","                  compute_metrics=compute_metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","import wandb\n","\n","user_secrets = UserSecretsClient()\n","\n","wandb_api = user_secrets.get_secret(\"wandb_api\")\n","\n","wandb.login(key=wandb_api)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wF_EGWQoyxuG","outputId":"47e5c12d-e7ec-4c54-9105-b310b6c1c7d1","trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DX0JjFUJMjLG","trusted":true},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FxF-AcGMzct","trusted":true},"outputs":[],"source":["model1.save_pretrained('/kaggle/working/model_camembert_V4/')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4125931,"sourceId":7147062,"sourceType":"datasetVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
